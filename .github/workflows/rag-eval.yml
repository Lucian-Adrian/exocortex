name: RAG Evaluation

on:
  pull_request:
    branches: [main, master]
    paths:
      - 'exo/**'
      - 'tests/rag/**'
  workflow_dispatch:
    inputs:
      threshold:
        description: 'Minimum score threshold (0.0-1.0)'
        required: false
        default: '0.7'

jobs:
  rag-evaluation:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[llmops,dev]"
      
      - name: Run RAG evaluation tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          pytest tests/rag/ -v --tb=short
      
      - name: Generate evaluation report
        if: always()
        run: |
          echo "## RAG Evaluation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Answer Relevancy | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
          echo "| Faithfulness | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
          echo "| Contextual Relevancy | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
